{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-01T14:34:45.302770Z","iopub.execute_input":"2025-04-01T14:34:45.303153Z","iopub.status.idle":"2025-04-01T14:34:45.309335Z","shell.execute_reply.started":"2025-04-01T14:34:45.303076Z","shell.execute_reply":"2025-04-01T14:34:45.308114Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T14:34:45.376594Z","iopub.execute_input":"2025-04-01T14:34:45.376942Z","iopub.status.idle":"2025-04-01T14:34:51.150723Z","shell.execute_reply.started":"2025-04-01T14:34:45.376913Z","shell.execute_reply":"2025-04-01T14:34:51.149312Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":79},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import HTML, Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T12:03:07.503574Z","iopub.execute_input":"2025-04-05T12:03:07.504008Z","iopub.status.idle":"2025-04-05T12:03:08.982120Z","shell.execute_reply.started":"2025-04-05T12:03:07.503966Z","shell.execute_reply":"2025-04-05T12:03:08.980940Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Set API key:","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T12:03:11.554347Z","iopub.execute_input":"2025-04-05T12:03:11.554818Z","iopub.status.idle":"2025-04-05T12:03:11.691121Z","shell.execute_reply.started":"2025-04-05T12:03:11.554754Z","shell.execute_reply":"2025-04-05T12:03:11.690054Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Run your first prompt","metadata":{}},{"cell_type":"code","source":"client = genai.Client(api_key=GOOGLE_API_KEY)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=\"Explain AI to me like I'm a kid.\")\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T12:03:17.734782Z","iopub.execute_input":"2025-04-05T12:03:17.735133Z","iopub.status.idle":"2025-04-05T12:03:19.691450Z","shell.execute_reply.started":"2025-04-05T12:03:17.735107Z","shell.execute_reply":"2025-04-05T12:03:19.690255Z"}},"outputs":[{"name":"stdout","text":"Okay, imagine you have a really, really smart puppy. This puppy can learn tricks, right? You teach it to sit, fetch, and roll over.\n\nAI is like that puppy, but instead of learning tricks from you, it learns from lots and lots of information! Think of it like showing the puppy millions of pictures of cats, and it learns what a cat looks like.\n\nSo, AI is a computer program that's been given lots and lots of information and taught to do something smart.\n\nHere are some examples:\n\n*   **Talking Robots like Siri or Alexa:** They've been taught to understand your voice and answer your questions, kind of like a really smart, helpful parrot!\n*   **Video Games:** Sometimes the bad guys in video games are controlled by AI. They learn from you playing and try to outsmart you!\n*   **Movie Recommendations on Netflix:** Netflix looks at all the movies you've watched and figures out what else you might like. It's like a movie-loving friend who always knows what you want to see!\n\nAI isn't magic, but it's a way of making computers smarter so they can help us with things. The more information we give them, the better they get at helping! So, just like training a puppy, training AI takes a lot of work!\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"**The response often comes back in markdown format, which you can render directly in this notebook.**","metadata":{}},{"cell_type":"code","source":"Markdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T14:34:54.326838Z","iopub.execute_input":"2025-04-01T14:34:54.327536Z","iopub.status.idle":"2025-04-01T14:34:54.335672Z","shell.execute_reply.started":"2025-04-01T14:34:54.327474Z","shell.execute_reply":"2025-04-01T14:34:54.334185Z"}},"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Okay, imagine you have a really smart puppy! This puppy can learn new tricks, but instead of using treats, it uses information. That's kind of what AI is!\n\nAI stands for Artificial Intelligence.  \"Artificial\" means it's not real, like a fake flower.  \"Intelligence\" means being able to learn and understand things, like you learning to read!\n\nSo, AI is like a computer program that can learn and do things that usually only smart people (or smart puppies!) can do.\n\n**Here's how it works:**\n\n1. **Lots and lots of examples!**  We show the AI tons and tons of examples.  Like, if we're teaching it to recognize pictures of cats, we show it thousands and thousands of pictures of cats!\n2. **Finding patterns!**  The AI looks at all those examples and tries to find patterns.  It notices that cats usually have pointy ears, whiskers, and furry bodies.\n3. **Making decisions!**  Once it knows the patterns, it can use them to make decisions.  So, if you show it a new picture, it can look for those cat patterns and say, \"Hey, that's probably a cat!\"\n\n**Think of it like this:**\n\n*   If you teach a computer all the rules of chess, it can learn to play chess really well! That's AI!\n*   If you show a computer lots of pictures of different kinds of flowers, it can learn to tell you what kind of flower you show it in a new picture! That's AI!\n\n**Where do we see AI?**\n\n*   **On your phone:**  Like when you use Siri or Google Assistant.\n*   **In video games:**  To make the characters you play against smarter.\n*   **On websites:**  To recommend things you might like to buy.\n*   **In self-driving cars:**  To help them drive without a person!\n\n**Is AI magic?**\n\nNo, it's not magic. It's just really, really clever programming.  It needs smart people to teach it and to help it learn.\n\n**Important things to remember:**\n\n*   AI is getting smarter all the time!\n*   AI is a tool, and like any tool, it can be used for good or bad.\n\nSo, that's AI in a nutshell!  It's like a super-smart, learning machine that can help us do all sorts of things! Do you have any other questions?\n"},"metadata":{}}],"execution_count":83},{"cell_type":"markdown","source":"## **Start a chat**","metadata":{}},{"cell_type":"code","source":"chat = client.chats.create(model='gemini-2.0-flash', history=[])\nresponse = chat.send_message('Hello! My name is Zlork.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T14:34:54.336871Z","iopub.execute_input":"2025-04-01T14:34:54.337216Z","iopub.status.idle":"2025-04-01T14:34:54.918327Z","shell.execute_reply.started":"2025-04-01T14:34:54.337185Z","shell.execute_reply":"2025-04-01T14:34:54.917044Z"}},"outputs":[{"name":"stdout","text":"Hello Zlork! It's nice to meet you. How can I help you today?\n\n","output_type":"stream"}],"execution_count":84},{"cell_type":"markdown","source":"## **another chat**","metadata":{}},{"cell_type":"code","source":"response = chat.send_message('Can you tell me something interesting about dinosaurs?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T14:34:54.919545Z","iopub.execute_input":"2025-04-01T14:34:54.919894Z","iopub.status.idle":"2025-04-01T14:34:56.191609Z","shell.execute_reply.started":"2025-04-01T14:34:54.919844Z","shell.execute_reply":"2025-04-01T14:34:56.190349Z"}},"outputs":[{"name":"stdout","text":"Okay, here's a fascinating fact about dinosaurs:\n\n**Many dinosaurs were covered in feathers, not just scales!**\n\nWhile we often picture dinosaurs as scaly reptiles, growing fossil evidence, especially from China, shows that many theropods (the group that includes Tyrannosaurus Rex and Velociraptor) had feathers. These feathers weren't necessarily for flight in all cases; they likely served purposes like insulation, display for mating, or camouflage.\n\nSo, the image of a T-Rex might be more accurate if it was covered in a downy coat of feathers rather than just tough scales!\n\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"response = chat.send_message('Do you remember what my name is?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T14:34:56.194474Z","iopub.execute_input":"2025-04-01T14:34:56.194944Z","iopub.status.idle":"2025-04-01T14:34:56.707699Z","shell.execute_reply.started":"2025-04-01T14:34:56.194895Z","shell.execute_reply":"2025-04-01T14:34:56.706708Z"}},"outputs":[{"name":"stdout","text":"Yes, your name is Zlork.\n\n","output_type":"stream"}],"execution_count":86},{"cell_type":"markdown","source":"## **Choose a model**\nThe Gemini API provides access to a number of models from the Gemini model family. Read about the available models and their capabilities on the model overview page.\nIn this step we'll use the API to list all of the available models.\n\n**Available Gemini models (2024):**  \n`gemini-1.5-flash` (fastest), `gemini-1.5-pro` (balanced), `gemini-2.0-flash`, `gemini-pro-vision` (multimodal), and `gemini-1.0-pro` (legacy). Use with `models/` prefix (e.g., `models/gemini-1.5-pro`). Full list may vary by region/access.  ","metadata":{}},{"cell_type":"code","source":"for model in client.models.list():\n  print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T14:34:56.709299Z","iopub.execute_input":"2025-04-01T14:34:56.709707Z","iopub.status.idle":"2025-04-01T14:34:56.878198Z","shell.execute_reply.started":"2025-04-01T14:34:56.709665Z","shell.execute_reply":"2025-04-01T14:34:56.876881Z"}},"outputs":[],"execution_count":87},{"cell_type":"markdown","source":"# Controlling Output Length in Gemini API","metadata":{}},{"cell_type":"code","source":"short_config = types.GenerateContentConfig(max_output_tokens=200)\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents='Write a 1000 word essay on the importance of olives in modern society.')\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T15:11:37.320567Z","iopub.execute_input":"2025-04-01T15:11:37.320950Z","iopub.status.idle":"2025-04-01T15:11:38.746797Z","shell.execute_reply.started":"2025-04-01T15:11:37.320912Z","shell.execute_reply":"2025-04-01T15:11:38.745678Z"}},"outputs":[{"name":"stdout","text":"## The Humble Olive: A Cornerstone of Modern Society\n\nThe olive, a fruit borne from the ancient olive tree (Olea europaea), often evokes images of sun-drenched Mediterranean landscapes, tranquil groves, and timeless traditions. While these romantic notions hold true, the olive's significance extends far beyond picturesque aesthetics. From its crucial role in global cuisine and its contributions to human health to its impact on sustainable agriculture and its cultural importance, the olive is, undeniably, a cornerstone of modern society.\n\nOne of the most palpable contributions of the olive lies in the realm of gastronomy. Olive oil, extracted from the fruit, has become a ubiquitous and essential ingredient in kitchens worldwide. Its versatility is unparalleled. It serves as a base for countless dishes, from simple salads and pasta sauces to complex stews and roasts. Its diverse range of flavors, influenced by variety, terroir, and processing methods, allows chefs and home cooks alike to achieve nuanced and distinctive tastes. Extra virgin olive oil, the highest\n","output_type":"stream"}],"execution_count":98},{"cell_type":"markdown","source":"## Generating a Short Poem Using Gemini 2.0 Flash","metadata":{}},{"cell_type":"code","source":"response = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents='Write a short poem on the importance of olives in modern society.')\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T15:12:11.068544Z","iopub.execute_input":"2025-04-01T15:12:11.068925Z","iopub.status.idle":"2025-04-01T15:12:12.071264Z","shell.execute_reply.started":"2025-04-01T15:12:11.068890Z","shell.execute_reply":"2025-04-01T15:12:12.070158Z"}},"outputs":[{"name":"stdout","text":"From sun-drenched groves, a humble fruit,\nAn olive comes, in brine imbued.\nNo longer just a Grecian treat,\nBut global fare, on plates reviewed.\n\nIn tapenade, or pressed for oil,\nA Mediterranean's liquid grace.\nEnhancing dishes, rich in toil,\nIt leaves its mark on every place.\n\nA symbol too, of peace and health,\nA staple crop, a farmer's pride.\nThe olive stands, a quiet wealth,\nIn modern life, a constant guide.\n\n","output_type":"stream"}],"execution_count":99},{"cell_type":"markdown","source":"## Generating Random Colors with High Temperature Setting\n\n**Temperature** is a setting in language models like Gemini that determines how random the selection of words or tokens will be. If you increase the temperature (e.g., to 2.0), the model chooses from a wider range of possible next tokens, making the output more diverse and creative. However, if you lower the temperature (e.g., to 0), the model always picks the most probable token, which is called \"**greedy decoding**.\"","metadata":{}},{"cell_type":"code","source":"high_temp_config = types.GenerateContentConfig(temperature=2.0)\n\n\nfor _ in range(5):\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=high_temp_config,\n      contents='Pick a random colour... (respond in a single word)')\n\n  if response.text:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T15:17:55.127449Z","iopub.execute_input":"2025-04-01T15:17:55.127832Z","iopub.status.idle":"2025-04-01T15:17:56.690206Z","shell.execute_reply.started":"2025-04-01T15:17:55.127802Z","shell.execute_reply":"2025-04-01T15:17:56.688818Z"}},"outputs":[{"name":"stdout","text":"Turquoise\n -------------------------\nAzure.\n -------------------------\nTurquoise\n -------------------------\nCerulean\n -------------------------\nAzure\n -------------------------\n","output_type":"stream"}],"execution_count":100},{"cell_type":"code","source":"low_temp_config = types.GenerateContentConfig(temperature=0.0)\n\nfor _ in range(5):\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=low_temp_config,\n      contents='Pick a random colour... (respond in a single word)')\n\n  if response.text:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T15:29:12.408595Z","iopub.execute_input":"2025-04-01T15:29:12.408975Z","iopub.status.idle":"2025-04-01T15:29:13.731519Z","shell.execute_reply.started":"2025-04-01T15:29:12.408937Z","shell.execute_reply":"2025-04-01T15:29:13.730428Z"}},"outputs":[{"name":"stdout","text":"Azure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\n","output_type":"stream"}],"execution_count":102},{"cell_type":"markdown","source":"# **Top-P**\n\n#### **How does Top-P control the diversity of a language model's output and how does it differ from Temperature or Top-K?**\n\n**Top-P** is another setting for managing text diversity in language models like Gemini. This parameter sets a probability threshold - once the cumulative probability of tokens exceeds it, no additional tokens are considered for selection.","metadata":{}},{"cell_type":"code","source":"model_config = types.GenerateContentConfig(\n    # These are the default values for gemini-2.0-flash.\n    temperature=1.0,\n    top_p=0.95,\n)\n\nstory_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=story_prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T15:30:00.318505Z","iopub.execute_input":"2025-04-01T15:30:00.318862Z","iopub.status.idle":"2025-04-01T15:30:05.818532Z","shell.execute_reply.started":"2025-04-01T15:30:00.318831Z","shell.execute_reply":"2025-04-01T15:30:05.817517Z"}},"outputs":[{"name":"stdout","text":"Jasper wasn't your average tabby. Sure, he enjoyed naps in sunbeams and the occasional feathered toy, but a restless spirit thrummed beneath his striped fur. He yearned for adventure, a world beyond the familiar scent of his human, Mrs. Higgins, and the predictable rhythm of her afternoon tea.\n\nOne Tuesday, fueled by a particularly potent dream of chasing butterflies the size of birds, Jasper decided enough was enough. He waited until Mrs. Higgins was engrossed in her soap opera, then, with a flick of his tail, slipped out the slightly ajar back door.\n\nThe world exploded with newness. The air hummed with the buzzing of bees, the earth smelled rich and damp, and the lawn was a jungle of towering grass blades. He cautiously padded forward, his senses on high alert.\n\nHis adventure began with a perilous trek across the gravel driveway, dodging the tires of the mailman’s truck with feline agility. He then scaled a towering oak tree, his claws digging into the rough bark, reaching a vantage point he’d only dreamed of. The world stretched before him: rooftops, gardens, and a tantalizing glimpse of a shimmering river.\n\nHe decided the river was his destination. \n\nThe journey was fraught with peril. A grumpy bulldog named Brutus, chained to a fence, barked a furious warning. Jasper, wise to the ways of dogs, nonchalantly licked his paw and pretended not to notice, before disappearing into a thicket of rose bushes. He navigated a bustling street, dodging speeding cars with a nerve-wracking mix of luck and instinct.\n\nFinally, he reached the riverbank. The air was cool and damp, and the water sparkled under the afternoon sun. He cautiously approached, dipping a paw into the cool, rushing current. This was it, the adventure he’d craved.\n\nThen he saw it. A small, bright yellow rubber ducky, bobbing along in the current. It was gloriously, wonderfully out of place. He watched it, mesmerized, his hunter's instinct taking over.\n\nHe crouched, his tail twitching. He calculated the trajectory, the distance, the leap. This wasn't just a rubber ducky, it was a symbol of his grand adventure, a prize to bring back to Mrs. Higgins, a token of his bravery.\n\nHe leaped.\n\nHe landed squarely on the ducky, which, of course, promptly capsized. Jasper, surprised and slightly damp, clung to the slippery plastic, paddling furiously. He was swept downstream, the river rushing past, the adventure suddenly becoming a bit too adventurous.\n\nHe was scared, yes, but also exhilaratingly alive. He was no longer just Jasper, the house cat. He was Jasper, the river-faring adventurer!\n\nEventually, he managed to paddle towards a grassy bank and haul himself ashore, shivering and bedraggled, but clutching the slightly chewed rubber ducky.\n\nAs dusk began to settle, he started the long trek home, his fur plastered to his body, the yellow ducky held triumphantly in his jaws.\n\nHe slipped back into the house through the same slightly ajar door, Mrs. Higgins still glued to her soap opera. He dropped the rubber ducky at her feet, a soggy offering from his grand adventure.\n\nMrs. Higgins gasped. \"Jasper! Where have you been? And where did you get… Quacky!\" She scooped him up, burying her face in his wet fur. He purred, a low rumble of contentment.\n\nHe might have been a little damp, a little scratched, and the adventure might not have gone exactly as planned, but he had seen the world, conquered his fears, and brought home a trophy. He was Jasper, the adventurer, and he was home. For now. He knew, with a thrilling certainty, that this was only the beginning. The world was vast, and there were still butterflies the size of birds to chase.\n\n","output_type":"stream"}],"execution_count":103},{"cell_type":"markdown","source":"## **Prompting**\nThis code uses a language model to classify the sentiment of a movie review into one of three categories: positive (POSITIVE), neutral (NEUTRAL), or negative (NEGATIVE). It employs a **# zero-shot** approach, meaning that the model makes predictions directly without having previous examples.\n\n## **Zero Shot**\n","metadata":{}},{"cell_type":"code","source":"model_config = types.GenerateContentConfig(\n    temperature=0.1,\n    top_p=1,\n    max_output_tokens=5,\n)\nzero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment: \"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=zero_shot_prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T15:41:35.944822Z","iopub.execute_input":"2025-04-01T15:41:35.945245Z","iopub.status.idle":"2025-04-01T15:41:36.438821Z","shell.execute_reply.started":"2025-04-01T15:41:35.945210Z","shell.execute_reply":"2025-04-01T15:41:36.437681Z"}},"outputs":[{"name":"stdout","text":"POSITIVE\n\n","output_type":"stream"}],"execution_count":104},{"cell_type":"markdown","source":"## **Few_Shot**\n\nTraining the model with predefined examples (**Few-Shot Learning**)","metadata":{}},{"cell_type":"code","source":"few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n```\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with cheese & pineapple\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ),\n    contents=[few_shot_prompt, customer_order])\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:11:04.975422Z","iopub.execute_input":"2025-04-01T16:11:04.975823Z","iopub.status.idle":"2025-04-01T16:11:05.559853Z","shell.execute_reply.started":"2025-04-01T16:11:04.975783Z","shell.execute_reply":"2025-04-01T16:11:05.558565Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"pineapple\"]\n}\n```\n\n","output_type":"stream"}],"execution_count":113},{"cell_type":"markdown","source":"# Chain of Thought - CoT\nThe purpose of this section is to describe the \"Chain of Thought\" (CoT) method, which aims to improve the accuracy of responses from large language models (LLMs) by providing intermediate reasoning steps.\n**Solves a simple math problem using the Gemini model**","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer directly.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:13:25.992361Z","iopub.execute_input":"2025-04-01T16:13:25.992723Z","iopub.status.idle":"2025-04-01T16:13:26.484875Z","shell.execute_reply.started":"2025-04-01T16:13:25.992693Z","shell.execute_reply":"2025-04-01T16:13:26.483573Z"}},"outputs":[{"name":"stdout","text":"52\n\n","output_type":"stream"}],"execution_count":115},{"cell_type":"markdown","source":"##### **Now try the same approach, but indicate to the model that it should \"think step by step\".**","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\nI am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:22:04.118188Z","iopub.execute_input":"2025-04-01T16:22:04.118574Z","iopub.status.idle":"2025-04-01T16:22:05.021138Z","shell.execute_reply.started":"2025-04-01T16:22:04.118544Z","shell.execute_reply":"2025-04-01T16:22:05.020018Z"}},"outputs":[{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Here's how to solve this:\n\n1.  **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n2.  **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n3.  **Determine partner's current age:** Since you are now 20, your partner is 20 + 8 = 28 years old.\n\n**Therefore, your partner is now 28 years old.**"},"metadata":{}}],"execution_count":116},{"cell_type":"markdown","source":"### ReAct: Reason and act\n\nA **question-answering** system based on the language model gemini-2.0-flash can also be implemented, utilizing the **ReAct** technique. ReAct is an intelligent approach to solving complex problems that involves sequential steps of Thought, Action, and Observation.","metadata":{}},{"cell_type":"code","source":"model_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) <finish>answer</finish>, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n<search>Milhouse</search>\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n<lookup>named after</lookup>\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n<finish>Richard Nixon</finish>\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n<search>Colorado orogeny</search>\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n<lookup>eastern sector</lookup>\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n<search>High Plains</search>\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n<search>High Plains (United States)</search>\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n<finish>1,800 to 7,000 ft</finish>\n\"\"\"\n\n# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:24:33.358144Z","iopub.execute_input":"2025-04-01T16:24:33.358576Z","iopub.status.idle":"2025-04-01T16:24:33.365624Z","shell.execute_reply.started":"2025-04-01T16:24:33.358542Z","shell.execute_reply":"2025-04-01T16:24:33.364080Z"}},"outputs":[],"execution_count":117},{"cell_type":"code","source":"question = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\n# You will perform the Action; so generate up to, but not including, the Observation.\nreact_config = types.GenerateContentConfig(\n    stop_sequences=[\"\\nObservation\"],\n    system_instruction=model_instructions + example1 + example2,\n)\n\n# Create a chat that has the model instructions and examples pre-seeded.\nreact_chat = client.chats.create(\n    model='gemini-2.0-flash',\n    config=react_config,\n)\n\nresp = react_chat.send_message(question)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:25:34.526885Z","iopub.execute_input":"2025-04-01T16:25:34.527447Z","iopub.status.idle":"2025-04-01T16:25:35.390780Z","shell.execute_reply.started":"2025-04-01T16:25:34.527402Z","shell.execute_reply":"2025-04-01T16:25:35.389657Z"}},"outputs":[{"name":"stdout","text":"Thought 1\nI need to find the transformers NLP paper and identify the authors and their ages to find the youngest one.\n\nAction 1\n<search>transformers NLP paper</search>\n\n","output_type":"stream"}],"execution_count":118},{"cell_type":"markdown","source":"**Now you can perform this research yourself and supply it back to the model.**","metadata":{}},{"cell_type":"code","source":"observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:37:52.368726Z","iopub.execute_input":"2025-04-01T16:37:52.369699Z","iopub.status.idle":"2025-04-01T16:37:53.401479Z","shell.execute_reply.started":"2025-04-01T16:37:52.369647Z","shell.execute_reply":"2025-04-01T16:37:53.400407Z"}},"outputs":[{"name":"stdout","text":"Thought 2\nThe paper lists the authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. Now I need to find their ages at the time of the paper. Since the paper's ID is 1706.03762, I can assume it was published in 2017.\n\nAction 2\n<search>Ashish Vaswani age</search>\n\n","output_type":"stream"}],"execution_count":119},{"cell_type":"markdown","source":"## **Thinking mode**\nThe purpose of this section is to introduce and explain the \"**Thinking Mode**\" feature in the Gemini Flash 2.0 prototype.","metadata":{}},{"cell_type":"code","source":"import io\nfrom IPython.display import Markdown, clear_output\n\nresponse = client.models.generate_content_stream(\n    model='gemini-2.0-flash-thinking-exp',\n    contents='Who was the youngest author listed on the transformers NLP paper?',)\n\nbuf = io.StringIO()\nfor chunk in response:\n    buf.write(chunk.text)\n    # Display the response as it is streamed\n    print(chunk.text, end='')\n\n# And then render the finished response as formatted markdown.\nclear_output()\nMarkdown(buf.getvalue())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:41:26.871806Z","iopub.execute_input":"2025-04-01T16:41:26.872214Z","iopub.status.idle":"2025-04-01T16:41:40.835335Z","shell.execute_reply.started":"2025-04-01T16:41:26.872181Z","shell.execute_reply":"2025-04-01T16:41:40.834112Z"}},"outputs":[{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"The youngest author listed on the \"Attention is All You Need\" paper, which introduced the Transformer architecture, is **Aidan N. Gomez**.\n\nHe was born in 1995.  The paper was published in 2017. This would make him approximately **22 years old** at the time of publication."},"metadata":{}}],"execution_count":122},{"cell_type":"markdown","source":"# Code prompting\n## **Generating code**\nThis section describes the Gemini family of models' ability to generate code, settings, and scripts to show how it can be used to learn programming, master new languages, or quickly draft code.","metadata":{}},{"cell_type":"code","source":"# The Gemini models love to talk, so it helps to specify they stick to the code if that\n# is all that you want.\ncode_prompt = \"\"\"\nWrite a Python function to calculate the factorial of a number. No explanation, provide only the code.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=1,\n        top_p=1,\n        max_output_tokens=1024,\n    ),\n    contents=code_prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:47:42.442269Z","iopub.execute_input":"2025-04-01T16:47:42.442664Z","iopub.status.idle":"2025-04-01T16:47:43.276999Z","shell.execute_reply.started":"2025-04-01T16:47:42.442629Z","shell.execute_reply":"2025-04-01T16:47:43.275897Z"}},"outputs":[{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef factorial(n):\n  \"\"\"\n  Calculate the factorial of a number.\n\n  Args:\n    n: The number to calculate the factorial of.\n\n  Returns:\n    The factorial of n.\n  \"\"\"\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```\n"},"metadata":{}}],"execution_count":123},{"cell_type":"markdown","source":"This response contains multiple parts, including an opening and closing text part that represent regular responses, an executable_code part that represents generated code and a code_execution_result part that represents the results from running the generated code.\n\nWe can explore them individually.","metadata":{}},{"cell_type":"code","source":"for part in response.candidates[0].content.parts:\n    if part.text:\n        display(Markdown(part.text))\n    elif part.executable_code:\n        display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n    elif part.code_execution_result:\n        if part.code_execution_result.outcome != 'OUTCOME_OK':\n            display(Markdown(f'## Status {part.code_execution_result.outcome}'))\n\n        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:55:39.725922Z","iopub.execute_input":"2025-04-01T16:55:39.726371Z","iopub.status.idle":"2025-04-01T16:55:39.741144Z","shell.execute_reply.started":"2025-04-01T16:55:39.726335Z","shell.execute_reply":"2025-04-01T16:55:39.739577Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Okay, I can do that. First, I'll list the first 14 odd prime numbers. Remember that a prime number is a number greater than 1 that has only two factors: 1 and itself. Also, by \"odd prime numbers,\" we're excluding 2, which is the only even prime.\n\nThe first few prime numbers are 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53...\n\nTherefore, the first 14 odd prime numbers are: 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47.\n\nNow, I will calculate their sum using python.\n\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\nimport numpy as np\n\nprimes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\nsum_of_primes = np.sum(primes)\n\nprint(f'{sum_of_primes=}')\n\n```"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```\nsum_of_primes=np.int64(326)\n\n```"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"The sum of the first 14 odd prime numbers (3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47) is 326.\n"},"metadata":{}}],"execution_count":131},{"cell_type":"markdown","source":"### Explaining code\nThe Gemini family of models can explain code to you too. In this example, you pass a bash script and ask some questions.","metadata":{}},{"cell_type":"code","source":"file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n\nexplain_prompt = f\"\"\"\nPlease explain what this file does at a very high level. What is it, and why would I use it?\n\n```\n{file_contents}\n```\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=explain_prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:57:57.962531Z","iopub.execute_input":"2025-04-01T16:57:57.962892Z","iopub.status.idle":"2025-04-01T16:58:01.297554Z","shell.execute_reply.started":"2025-04-01T16:57:57.962864Z","shell.execute_reply":"2025-04-01T16:58:01.296668Z"}},"outputs":[{"execution_count":132,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"This file, `git-prompt.sh`, is a shell script designed to enhance your command-line prompt by displaying information about the current Git repository. In simple terms, it makes your prompt \"git-aware\".\n\nHere's a breakdown:\n\n*   **What it does:** It modifies your command prompt to show the current Git branch, the status of your working directory (e.g., staged changes, untracked files, ahead/behind remote), and other relevant Git information. It uses colors to visually highlight different states.\n\n*   **Why use it?** This script gives you at-a-glance information about the state of your Git repository right in your command prompt, without needing to run `git status` manually. This improves your workflow and reduces errors. You can quickly see if you have uncommitted changes, if you're behind the remote branch, etc.\n\n*   **How it works:** The script defines functions that:\n\n    *   Detect if the current directory is a Git repository.\n    *   Gather information about the Git repository's status using `git` commands.\n    *   Format this information into a string with colors and symbols.\n    *   Update the `PS1` environment variable (which controls the command prompt) to include this Git information. It utilizes functions to load theme and color settings.\n    * It's designed to be compatible with both `bash` and `zsh`.\n\n*   **Installation:** To use it, you would typically source this script in your `.bashrc` (for Bash) or `.zshrc` (for Zsh) file.  For example:\n\n```bash\nsource /path/to/git-prompt.sh\n```\n\nThe script also provides functions for customizing the look and feel of the prompt, such as changing the colors, symbols, and which information is displayed. In addition, the `gp_install_prompt` function attempts to automatically setup the prompt within your shell.\n\nIn essence, `git-prompt.sh` is a productivity tool that integrates Git status information into your command-line prompt, making it easier to manage your Git repositories.\n"},"metadata":{}}],"execution_count":132},{"cell_type":"markdown","source":"The goal of this section is to provide comprehensive and practical resources for a deeper study of prompt writing and using the Gemini API so that users can expand their knowledge and take advantage of the available tools and examples.","metadata":{}}]}